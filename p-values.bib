%$Id: APSampler-open.bib 1264 2010-07-09 20:12:56Z favorov $
@article{bland_statistics_2000,
	title = {Statistics notes. The odds ratio},
	volume = {320},
	issn = {0959-8138},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/10827061},
	number = {7247},
	journal = {{BMJ} {(Clinical} Research Ed.)},
	author = {J M Bland and D G Altman},
	month = may,
	year = {2000},
	note = {{PMID:} 10827061},
	keywords = {Odds Ratio},
	pages = {1468}
},

@book{westfall_resampling-based_1993,
	title = {Resampling-based multiple testing},
	isbn = {0471557617, 9780471557616},
	publisher = {John Wiley and Sons},
	author = {Peter H. Westfall and S. Stanley Young},
	year = {1993},
	pages = {382}
},

@article{benjamini_controllingfalse_1995,
	title = {Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing},
	volume = {57},
	issn = {00359246},
	shorttitle = {Controlling the False Discovery Rate},
	url = {http://www.jstor.org/stable/2346101},
	abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate {(FWER).} This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses-the false discovery rate. This error rate is equivalent to the {FWER} when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the {FWER} is desired, there is potential for a gain in power. A simple sequential Bonferroni-type procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
	number = {1},
	journal = {Journal of the Royal Statistical Society. Series B {(Methodological)}},
	author = {Yoav Benjamini and Yosef Hochberg},
	year = {1995},
	note = {{ArticleType:} primary\_article / Full publication date: 1995 / Copyright © 1995 Royal Statistical Society},
	pages = {289--300}
},

@article{storey_statistical_2003,
	title = {Statistical significance for genomewide studies},
	volume = {100},
	issn = {0027-8424},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/12883005},
	doi = {10.1073/pnas.1530509100},
	abstract = {With the increase in genomewide experiments and the sequencing of multiple genomes, the analysis of large data sets has become commonplace in biology. It is often the case that thousands of features in a genomewide data set are tested against some null hypothesis, where a number of features are expected to be significant. Here we propose an approach to measuring statistical significance in these genomewide studies based on the concept of the false discovery rate. This approach offers a sensible balance between the number of true and false positives that is automatically calibrated and easily interpreted. In doing so, a measure of statistical significance called the q value is associated with each tested feature. The q value is similar to the well known p value, except it is a measure of significance in terms of the false discovery rate rather than the false positive rate. Our approach avoids a flood of false positive results, while offering a more liberal criterion than what has been used in genome scans for linkage.},
	number = {16},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {John D Storey and Robert Tibshirani},
	month = aug,
	year = {2003},
	note = {{PMID:} 12883005},
	keywords = {Algorithms, Alternative Splicing, Animals, Binding Sites, Exons, Gene Expression Regulation, Genetic Techniques, Genome, Humans, Linkage {(Genetics),} Oligonucleotide Array Sequence Analysis, Statistics as Topic, Transcription, Genetic},
	pages = {9440--9445}
},

@article{bland_multiple_1995,
	title = {Multiple significance tests: the Bonferroni method},
	volume = {310},
	issn = {0959-8138},
	shorttitle = {Multiple significance tests},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/7833759},
	number = {6973},
	journal = {{BMJ} {(Clinical} Research Ed.)},
	author = {J M Bland and D G Altman},
	year = {1995},
	note = {{PMID:} 7833759},
	keywords = {Clinical Trials as Topic, Data Interpretation, Statistical, Humans, Probability},
	pages = {170}
},

@article{skilling_nested_2006,
	title = {Nested sampling for general bayesian computation},
	volume = {1},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/euclid.ba/1340370944},
	doi = {10.1214/06-BA127},
	abstract = {Nested sampling estimates directly how the likelihood function relates to prior mass. The evidence (alternatively the marginal likelihood, marginal density of the data, or the prior predictive) is immediately obtained by summation. It is the prime result of the computation, and is accompanied by an estimate of numerical uncertainty. Samples from the posterior distribution are an optional by-product, obtainable for any temperature. The method relies on sampling within a hard constraint on likelihood value, as opposed to the softened likelihood of annealing methods. Progress depends only on the shape of the "nested" contours of likelihood, and not on the likelihood values. This invariance (over monotonic re-labelling) allows the method to deal with a class of phase-change problems which effectively defeat thermal annealing.},
	language = {EN},
	number = {4},
	urldate = {2020-08-13},
	journal = {Bayesian Analysis},
	author = {Skilling, John},
	month = dec,
	year = {2006},
	mrnumber = {MR2282208},
	zmnumber = {1332.62374},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {algorithm, annealing, Bayesian computation, evidence, marginal likelihood, model selection, nest, phase change},
	pages = {833--859},
}
